{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer, PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, plot_confusion_matrix\n",
    "\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://twitter.com/login\"\n",
    "#driver = webdriver.Chrome(executable_path=r\"D:\\chromedriver_win32\\\\chromedriver.exe\")\n",
    "driver = webdriver.Firefox(executable_path=r\"D:\\geckodriver.exe\")\n",
    "driver.get(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "username=driver.find_element_by_xpath('//*[@id=\"layers\"]/div[2]/div/div/div/div/div/div[2]/div[2]/div/div/div[2]/div[2]/div[1]/div/div[5]/label/div/div[2]/div/input')\n",
    "username.send_keys('rupam_27'+Keys.ENTER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "userpass=driver.find_element_by_name(\"password\")\n",
    "userpass.send_keys('orop8t702'+Keys.ENTER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "search=driver.find_element_by_xpath('//*[@id=\"react-root\"]/div/div/div[2]/main/div/div/div/div[2]/div/div[2]/div/div/div/div[1]/div/div/div/form/div[1]/div/div/label/div[2]/div/input')\n",
    "search.send_keys(\"pandemic birthrate\"+Keys.ENTER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(2)  # Allow 2 seconds for the web page to open\n",
    "scroll_pause_time = 1  # You can set your own pause time. My laptop is a bit slow so I use 1 sec\n",
    "screen_height = driver.execute_script(\"return window.screen.height;\")   # get the screen height of the web\n",
    "i = 1\n",
    "n=1\n",
    "test_list=[]\n",
    "#while True:\n",
    "while n<=100:\n",
    "    # scroll one screen height each time\n",
    "    driver.execute_script(\"window.scrollTo(0, {screen_height}*{i});\".format(screen_height=screen_height, i=i))  \n",
    "    i += 1\n",
    "    n+=1\n",
    "    time.sleep(scroll_pause_time)\n",
    "    # update scroll height each time after scrolled, as the scroll height can change after we scrolled the page\n",
    "    scroll_height = driver.execute_script(\"return document.body.scrollHeight;\")  \n",
    "    # Break the loop when the height we need to scroll to is larger than the total scroll height\n",
    "    test=driver.find_elements_by_xpath('//div[@class=\"css-1dbjc4n\"]//div[@class=\"css-1dbjc4n\"]//div[@lang=\"en\"]')\n",
    "    for j in range(len(test)):\n",
    "        test_list.append(test[j].text)\n",
    "    if (screen_height) * i > scroll_height:\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=driver.find_elements_by_xpath('//div[@class=\"css-1dbjc4n\"]//div[@class=\"css-1dbjc4n\"]//div[@lang=\"en\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=pd.DataFrame(test_list,columns=['tweets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.drop_duplicates(inplace=True,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEW COVER: Because of the pandemic, half a mil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"U.S. birthrate hits lowest ever\" are you tell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Britain is currently facing a ‘baby shortage’ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The amazing thing is that people think having ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When Dr Parasaram answered a question re the b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>USA experienced a marginal decrease of 4% in b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>\"The spiraling costs of child care, health car...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>This is going to be very bad for America -&gt; Be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>The spiraling costs of child care, health care...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Naw, really? “spiraling costs of child care, h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweets\n",
       "0    NEW COVER: Because of the pandemic, half a mil...\n",
       "1    \"U.S. birthrate hits lowest ever\" are you tell...\n",
       "2    Britain is currently facing a ‘baby shortage’ ...\n",
       "3    The amazing thing is that people think having ...\n",
       "4    When Dr Parasaram answered a question re the b...\n",
       "..                                                 ...\n",
       "96   USA experienced a marginal decrease of 4% in b...\n",
       "97   \"The spiraling costs of child care, health car...\n",
       "98   This is going to be very bad for America -> Be...\n",
       "99   The spiraling costs of child care, health care...\n",
       "100  Naw, really? “spiraling costs of child care, h...\n",
       "\n",
       "[101 rows x 1 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a Term-Document Matrix\n",
    "# Remove stop words\n",
    "stop = stopwords.words('english')\n",
    "text = pd.DataFrame(text)\n",
    "textcol = text['tweets']\n",
    "textcol = textcol.apply(lambda x: \" \".join(x for x in x.split() if x not in stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      NEW COVER: Because pandemic, half million fewe...\n",
       "1      \"U.S. birthrate hits lowest ever\" telling peop...\n",
       "2      Britain currently facing ‘baby shortage’ – pan...\n",
       "3      The amazing thing people think fucking pandemi...\n",
       "4      When Dr Parasaram answered question birthrate ...\n",
       "                             ...                        \n",
       "96     USA experienced marginal decrease 4% birthrate...\n",
       "97     \"The spiraling costs child care, health care e...\n",
       "98     This going bad America -> Because pandemic, ha...\n",
       "99     The spiraling costs child care, health care ed...\n",
       "100    Naw, really? “spiraling costs child care, heal...\n",
       "Name: tweets, Length: 101, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textcol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rups_\\AppData\\Local\\Temp/ipykernel_12364/1843087779.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  textcol = textcol.str.replace(patternnum,'')\n",
      "C:\\Users\\rups_\\AppData\\Local\\Temp/ipykernel_12364/1843087779.py:6: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  textcol = textcol.str.replace(patternpunc,'')\n"
     ]
    }
   ],
   "source": [
    "# Remove numerical values\n",
    "patternnum = '\\b[0-9]+\\b'\n",
    "textcol = textcol.str.replace(patternnum,'')\n",
    "# Remove punctuation\n",
    "patternpunc = '[^\\w\\s]'\n",
    "textcol = textcol.str.replace(patternpunc,'')\n",
    "# Convert to lowercase\n",
    "textcol = textcol.apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "# Stem the words\n",
    "porstem = PorterStemmer()\n",
    "textcol = textcol.apply(lambda x: \" \".join([porstem.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rups_\\AppData\\Local\\Temp/ipykernel_12364/1908563498.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  textcol = textcol.str.replace(pat_http,'')\n"
     ]
    }
   ],
   "source": [
    "#remove https\n",
    "pat_http='^http?://'\n",
    "textcol = textcol.str.replace(pat_http,'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      new cover becaus pandem half million fewer bab...\n",
       "1      us birthrat hit lowest ever tell peopl want ba...\n",
       "2      britain current face babi shortag pandem promp...\n",
       "3      the amaz thing peopl think fuck pandem lower b...\n",
       "4      when dr parasaram answer question birthrat i r...\n",
       "                             ...                        \n",
       "96     usa experienc margin decreas 4 birthrat fuck p...\n",
       "97     the spiral cost child care health care educ al...\n",
       "98     thi go bad america becaus pandem half million ...\n",
       "99     the spiral cost child care health care educ gl...\n",
       "100    naw realli spiral cost child care health care ...\n",
       "Name: tweets, Length: 101, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textcol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['14', '146', '15', '158', '1588', '164', '1861', '19', '1918', '1970', '1980', '2017', '2019', '2020', '2021', '21', '247', '25', '300000', '36', '500k', '6th', '800000', '814', '840832', 'aaliyah', 'abbiekamin', 'abdl', 'abrupt', 'acceler', 'across', 'activist', 'actual', 'admit', 'adopt', 'affect', 'afford', 'after', 'aftermath', 'again', 'age', 'ajohnston12', 'aka', 'alabama', 'alarm', 'all', 'along', 'alreadi', 'alreadyfal', 'also', 'amaz', 'america', 'american', 'amid', 'among', 'and', 'answer', 'anvil', 'anyth', 'aphrodisiac', 'appear', 'arent', 'arous', 'articl', 'as', 'asahi', 'assum', 'assumpt', 'at', 'attent', 'attitud', 'avail', 'babi', 'babymak', 'back', 'bad', 'badli', 'bare', 'barkin', 'basic', 'be', 'bear', 'becaus', 'becom', 'befor', 'beg', 'begin', 'believ', 'benjamin', 'better', 'big', 'biggest', 'birth', 'birthrat', 'boom', 'born', 'both', 'bounc', 'bounceback', 'brain', 'breadbak', 'breed', 'brilliant', 'bring', 'britain', 'broader', 'brought', 'budget', 'build', 'bullish', 'bunintheoven', 'bushfir', 'busi', 'bust', 'but', 'bw', 'call', 'care', 'case', 'catastroph', 'caus', 'cavemen', 'cdcgov', 'chanc', 'chang', 'child', 'childbear', 'childbirth', 'childcar', 'children', 'china', 'choos', 'cite', 'civil', 'clear', 'climat', 'clint', 'clutch', 'cna', 'coincid', 'collaps', 'come', 'complet', 'compound', 'consult', 'contribut', 'control', 'conveni', 'convers', 'corona', 'coronapocolyps', 'coronaviru', 'cost', 'could', 'countri', 'cours', 'cover', 'covid', 'covid19', 'covidstricken', 'cray', 'crise', 'crisi', 'crpakaluk', 'cultur', 'current', 'curv', 'cut', 'daili', 'damag', 'dark', 'data', 'date', 'day', 'death', 'deathrat', 'deathrateseri', 'debt', 'decad', 'decis', 'declar', 'declin', 'decreas', 'deep', 'deepen', 'def', 'deflationari', 'delay', 'demand', 'demograph', 'demographi', 'depart', 'deplet', 'depopul', 'depress', 'derang', 'develop', 'diaper', 'diaperindustri', 'die', 'differ', 'dip', 'disast', 'discuss', 'disinvest', 'dive', 'divis', 'divorc', 'down', 'downward', 'dr', 'drive', 'drop', 'due', 'dure', 'dynam', 'eagl', 'earli', 'easier', 'easili', 'eastenddistrict', 'econom', 'economi', 'economist', 'educ', 'effect', 'effort', 'elect', 'element', 'empir', 'employ', 'end', 'england', 'enjoy', 'equat', 'essenti', 'eugen', 'europ', 'even', 'ever', 'evict', 'exact', 'expect', 'experienc', 'expert', 'explain', 'exponenti', 'extent', 'face', 'fact', 'fall', 'fallout', 'famili', 'familyunequ', 'fed', 'feder', 'fell', 'femin', 'fertil', 'fewer', 'fight', 'fill', 'find', 'first', 'fishi', 'flatten', 'flu', 'fmtnew', 'folk', 'for', 'forc', 'foreign', 'forget', 'four', 'francescotosto6', 'friendli', 'fuck', 'fuckin', 'full', 'fumio', 'fund', 'further', 'futur', 'gay', 'gener', 'geopolitc', 'gestur', 'get', 'given', 'global', 'globalchang', 'go', 'good', 'govern', 'gradual', 'graph', 'great', 'greater', 'greatest', 'gregstock', 'grey', 'grow', 'growth', 'gruel', 'guarante', 'guardian', 'guzzo', 'gynaecologist', 'ha', 'half', 'hamper', 'happi', 'hard', 'hasten', 'have', 'he', 'head', 'health', 'healthcar', 'heard', 'help', 'here', 'hey', 'high', 'higher', 'highest', 'histori', 'hit', 'hobbi', 'home', 'honestli', 'horn', 'hospit', 'hous', 'how', 'httpdlvritrsnpkm', 'httpdomesticproductnet', 'httpsbbccomnewsworldasiaindia57759807', 'httpsbloombergcomnewsarticles20200729coronaviruspandemicamericansarentmakingbabiesincrisi', 'httpsbuffly2cnpkbr', 'httpscbccanewshealthcovid19birthrate15670539', 'httpscompleteintelcom20210622thedeathofgrowtholdrichvsyoungpoorin2030beyond', 'httpslatestlycomagencynewsworldnewsjapansbirthratefelltorecordlowin2020amidcovid192524433html', 'httpsnytimescom20210505ususbirthratefallscovidhtmlsmidtwshar', 'httpsnytimescom20210918worldcovidalabamadeathsbirthratehtml', 'httpsnytims33dwl3v', 'httpsonwsjcom2afzp9w', 'httpssmvorglearnbloghowhaspandemicimpactedbirthr', 'httpsthelilycomthepandemicishaltingsomewomenschildbearingplan', 'httpsthesarniajournalcababyboompandemichasbroughtariseinlocalbirthrateathospitalandinhom', 'httpstimecom6046065pandemicbirthr', 'httpstimecom6046065pandemicbirthratetextin20total2c20there20were2036number20of20births20in201980textsince2020142c20the20last20year22520fewer20births20per20year', 'httpstribaltj6jneh', 'httpstribalzkfttwd', 'httpswsjcomarticlesuspopulationgrowthslowsbirthratedeclineeconomicrisk11627231536nsprodaccountswsj', 'httpswww3nhkorjpnhkworldennewsvideos20200515113009799', 'httpsyenisafakcomenworldcovid19pandemichitsbirthrateinjapan3552309', 'httpvaacomtr2013702', 'httpwindowoneurasia2blogspotcom202007becausepandemichasdepressedhtml', 'huggi', 'human', 'humor', 'husband', 'idea', 'idkth', 'if', 'im', 'imagin', 'immigr', 'immunolog', 'impact', 'implic', 'improv', 'in', 'includ', 'incom', 'increas', 'india', 'inflat', 'inhom', 'instabl', 'instagram', 'interest', 'interperson', 'intro', 'invest', 'irk', 'irrelev', 'is', 'issu', 'it', 'itali', 'italian', 'itll', 'januari', 'japan', 'jesusisthelight', 'job', 'josh', 'juli', 'julianmig', 'just', 'karen', 'kearney', 'kearney_melissa', 'keep', 'kerala', 'key', 'kgeee', 'kid', 'kill', 'kind', 'kishida', 'know', 'labor', 'labour', 'laid', 'landlord', 'last', 'later', 'latest', 'lawyerfriend', 'ld', 'lead', 'leadership', 'lean', 'leap', 'learn', 'leav', 'led', 'lesser', 'let', 'level', 'levin', 'life', 'like', 'listen', 'literaci', 'live', 'livelihood', 'load', 'lock', 'lockdown', 'lol', 'long', 'longterm', 'loss', 'loui', 'low', 'lower', 'lowest', 'made', 'make', 'manag', 'mani', 'march', 'margin', 'market', 'marketingtrend', 'marketwatch', 'marriag', 'mask', 'massiv', 'mate', 'matern', 'matter', 'may', 'mayb', 'me', 'mean', 'media', 'meet', 'melissa', 'men', 'middl', 'million', 'minist', 'model', 'moment', 'month', 'more', 'mortal', 'mother', 'motherhood', 'move', 'much', 'mum', 'my', 'nation', 'naw', 'nazgolbagheri', 'nbclx', 'need', 'never', 'new', 'newborn', 'news', 'next', 'nine', 'no', 'nonexist', 'not', 'note', 'novelti', 'now', 'number', 'nytim', 'obstetrician', 'obvious', 'of', 'off', 'offici', 'often', 'one', 'onechild', 'opinion', 'opposit', 'or', 'otherwis', 'ought', 'our', 'outlast', 'ovari', 'overdr', 'pamper', 'pandem', 'pandemicbabi', 'pandemicbatt', 'pandemicrel', 'par', 'parasaram', 'parent', 'parti', 'parttim', 'pay', 'pearl', 'peep', 'pension', 'peopl', 'per', 'percent', 'person', 'perspect', 'pew', 'phd', 'philadelphia', 'philli', 'phillip', 'piec', 'place', 'plan', 'planet', 'play', 'plenti', 'plight', 'plummet', 'plung', 'point', 'polici', 'policymak', 'polit', 'poll', 'popul', 'pose', 'post', 'postpandem', 'potenti', 'power', 'ppl', 'predict', 'prepar', 'presid', 'prevent', 'prime', 'prioriti', 'probabl', 'problem', 'product', 'professor', 'promis', 'prompt', 'properti', 'proport', 'prospect', 'prove', 'push', 'put', 'putin', 'quarantin', 'question', 'quickhit', 'rapidli', 'rate', 'read', 'realli', 'rear', 'reason', 'recent', 'record', 'reduc', 'reeduc', 'referenc', 'region', 'relationship', 'reli', 'relief', 'remain', 'rememb', 'remot', 'renew', 'rephras', 'replac', 'requir', 'respond', 'respons', 'result', 'rethink', 'rework', 'richmond', 'richzeoli', 'right', 'rise', 'risen', 'risk', 'rn', 'role', 'run', 'russia', 'russian', 'rva', 'said', 'sanguin', 'sardinia', 'sarnia', 'sarniajourn', 'save', 'say', 'schedul', 'school', 'scienc', 'scientist', 'screenshot', 'screw', 'see', 'seek', 'seem', 'selfish', 'seriou', 'set', 'sever', 'shelv', 'shimbun', 'shock', 'short', 'shortag', 'show', 'shrink', 'signific', 'significantli', 'sinc', 'sixth', 'sizabl', 'slont', 'slow', 'slump', 'so', 'social', 'societi', 'some', 'someth', 'spaniard', 'spike', 'spiral', 'st', 'stabl', 'stagger', 'start', 'startup', 'state', 'stay', 'steeper', 'stigma', 'still', 'stop', 'store', 'straight', 'strategi', 'stream', 'street', 'strict', 'strife', 'struggl', 'student', 'studi', 'stupid', 'suck', 'suggest', 'sunk', 'superbowl', 'suppli', 'support', 'tackl', 'take', 'talk', 'talli', 'tax', 'technolog', 'tell', 'term', 'terribl', 'terrifi', 'than', 'that', 'the', 'then', 'theori', 'there', 'these', 'theyr', 'thi', 'thing', 'think', 'thought', 'throughout', 'thrown', 'thu', 'time', 'tip', 'to', 'told', 'total', 'toward', 'travel', 'trend', 'tri', 'true', 'truth', 'turn', 'twin', 'uighur', 'unborn', 'uncertainti', 'undermin', 'understand', 'unif', 'unit', 'unpecon', 'us', 'usa', 'usbirthr', 'use', 'vaccin', 'variabl', 'vc', 'via', 'video', 'vladimir', 'vow', 'vs', 'wa', 'wait', 'wake', 'wale', 'wall', 'want', 'war', 'washington', 'way', 'wear', 'week', 'weird', 'well', 'were', 'what', 'when', 'whi', 'which', 'while', 'whoa', 'win', 'window', 'wive', 'woman', 'women', 'wonder', 'word', 'work', 'workforc', 'world', 'worldwid', 'wors', 'worst', 'would', 'wouldb', 'wow', 'write', 'wwii', 'xink', 'yall', 'year', 'yet', 'young', 'your']\n"
     ]
    }
   ],
   "source": [
    "# Convert data into a document matrix\n",
    "vectorizer = CountVectorizer()\n",
    "tokens = pd.DataFrame(vectorizer.fit_transform(textcol).toarray(), columns=vectorizer.get_feature_names())\n",
    "tokens.columns\n",
    "print(tokens.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(n_components=4, random_state=35)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA\n",
    "vectorizer = CountVectorizer(max_df=0.8, min_df=4, stop_words='english')\n",
    "tweet_values = textcol.values.astype('U') #convert Panda values to unicode\n",
    "doc_term_matrix = vectorizer.fit_transform(tweet_values)\n",
    "doc_term_matrix.shape\n",
    "LDA = LatentDirichletAllocation(n_components=4, random_state=35)\n",
    "LDA.fit(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for topic #0:\n",
      "['immigr', 'year', 'coronaviru', 'push', 'popul', 'rise', 'say', 'think', 'econom', 'declin']\n",
      "\n",
      "\n",
      "Top 10 words for topic #1:\n",
      "['year', 'ha', 'peopl', '2020', 'japan', 'acceler', 'covid19', 'birth', 'drop', 'declin']\n",
      "\n",
      "\n",
      "Top 10 words for topic #2:\n",
      "['becaus', 'boom', 'guarante', 'half', 'born', 'bounc', 'million', 'fewer', '2021', 'babi']\n",
      "\n",
      "\n",
      "Top 10 words for topic #3:\n",
      "['educ', 'spiral', 'climat', 'includ', 'global', 'cost', 'health', 'child', 'chang', 'care']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i,topic in enumerate(LDA.components_):\n",
    "    print(f'Top 10 words for topic #{i}:')\n",
    "    print([vectorizer.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for topic #0:\n",
      "['immigr', 'year', 'american', 'push', 'peopl', 'rise', 'think', 'econom', 'declin', 'say']\n",
      "\n",
      "\n",
      "Top 10 words for topic #1:\n",
      "['rate', 'year', 'ha', 'covid19', '2020', 'acceler', 'popul', 'birth', 'drop', 'declin']\n",
      "\n",
      "\n",
      "Top 10 words for topic #2:\n",
      "['becaus', 'boom', 'guarante', 'half', 'bounc', 'born', 'million', 'fewer', '2021', 'babi']\n",
      "\n",
      "\n",
      "Top 10 words for topic #3:\n",
      "['spiral', 'climat', 'includ', 'global', 'cost', 'health', 'child', 'coronaviru', 'chang', 'care']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,topic in enumerate(LDA.components_):\n",
    "    print(f'Top 10 words for topic #{i}:')\n",
    "    print([vectorizer.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.to_csv(\"tweets on pandemic birth rate.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "075939418c5fdf99d2cec6c11fee0acb3cf765a09d39ea260205cdbaa06003a7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
